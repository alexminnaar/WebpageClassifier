[D] Full understanding of Out-Off-Bag-Error in Random Forests : MachineLearning jump to content my subreddits edit subscriptions popular -all -random |  AskReddit -funny -news -todayilearned -videos -pics -worldnews -gifs -aww -gaming -movies -Showerthoughts -mildlyinteresting -television -Jokes -IAmA -OldSchoolCool -LifeProTips -science -explainlikeimfive -nottheonion -Music -dataisbeautiful -books -sports -TwoXChromosomes -Futurology -Art -EarthPorn -food -personalfinance -photoshopbattles -tifu -UpliftingNews -DIY -WritingPrompts -GetMotivated -gadgets -space -history -nosleep -askscience -Documentaries -creepy -philosophy -listentothis -InternetIsBeautiful -blog -announcementsmore »  MachineLearning comments Want to join? Log in or sign up in seconds.| English limit my search to r/MachineLearning use the following search parameters to narrow your results: subreddit:subreddit find submissions in "subreddit" author:username find submissions by "username" site:example.com find submissions from "example.com" url:text search for "text" in url selftext:text search for "text" in self post contents self:yes (or self:no) include (or exclude) self posts nsfw:yes (or nsfw:no) include (or exclude) results marked as NSFW e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... this post was submitted on 27 Jul 2017 5 points (79% upvoted) shortlink: remember mereset password login Submit a new link Submit a new text post MachineLearningsubscribeunsubscribe113,775 readers 475 users here now Rules For Posts +Research +Discussion +Project +News @slashML on Twitter AMAs: Google Brain Team (8/11/2016) The MalariaSpot Team (2/6/2016) OpenAI Research Team (1/9/2016) Nando de Freitas (12/26/2015) Andrew Ng and Adam Coates (4/15/2015) Jürgen Schmidhuber (3/4/2015) Geoffrey Hinton (11/10/2014) Michael Jordan (9/10/2014) Yann LeCun (5/15/2014) Yoshua Bengio (2/27/2014) Beginners: Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try /r/LearnMachineLearning , /r/MLQuestions or http://stackoverflow.com/ Advanced Courses Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions /r/MLjobs and /r/BigDataJobs /r/datacleaning /r/DataScience /r/scientificresearch /r/artificial created by kunjaana community for 8 years message the moderators MODERATORS kunjaan cavedavenaive olaf_nij BeatLeJuce about moderation team » discussions in r/MachineLearning <> X 54 · 10 comments [P] Evolution Strategies in Keras 10 [R] Better Exploration with Parameter Noise 14 · 7 comments [D] PhD Student / Professor Assistant Experiences 16 [R] DARLA: Improving Zero-Shot Transfer in Reinforcement Learning 32 · 1 comment [R] Decision Forests that preserve personal privacy 14 · 4 comments [R] Comparative analysis of Chatbot modern platforms 128 · 16 comments [P] 5 puzzles about statistics that should be accessible to anyone without being trivial 5 [P] Sciblox version 2 updated! Cool new features, faster BPCA impute and more!!! 7 · 4 comments [D] Full understanding of Out-Off-Bag-Error in Random Forests 25 · 6 comments [R] Squeeze-and-Excitation networks, ILSVRC 2017 winner, at CVPR2017 (photos of presentation) 4 5 6 Discussion[D] Full understanding of Out-Off-Bag-Error in Random Forests (self.MachineLearning) submitted 8 hours ago by Dremet I do have some very detailed questions about the out-of-bag error in the random forest algorithm, which I couldn't answer after some research. My understanding: OOB error is the error resulting in going through each sample and calculating the accuracy by a vote of those trees, which did not get that sample in the bootstrapping procedure and are therefore not influenced by that very sample. This is done for every sample to estimate the overall accuracy. On "https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm" in the section "The out-of-bag (oob) error estimate" it says that there is no need for a separate test set. Assuming this is true, one can take the whole dataset for training and one can even estimate the expected error as well. What if you want to play around with some parameters like depth or amount of trees? I would guess you need a test set in that case and the OOB error within the training set tells you which parameters you should use, but you can't use the OOB error of the best parameters within the training set to estimate the expected error because you used all the data to determine the parameters. So in that case a separate test set, which was not involved in the parameter optimization, would be needed, correct? 4 comments share report all 4 comments sorted by: best topnewcontroversialoldrandomq&alive (beta) [–]vamany 1 point2 points3 points 7 hours ago (1 child) Taking a look at what is done in "Introduction to Statistical Learning", the authors demonstrate the use of both OOB error and test set error for parameter tuning. Check out Figure 8.8 in the book. In the figure, you can see that the OOB and test set errors can be different. I don't believe there are any guarantees for which one is more likely to be correct. However, the authors state that OOB can be shown to be almost equivalent to leave-one-out-cross-validation, but without the computational burden. And the computation time seems to be the primary factor in deciding which method to use to estimate error. As the authors state: "The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous." permalink embed save report give gold reply [–]Dremet[S] 0 points1 point2 points 7 hours ago (0 children) Thanks for your reply! This is my take-home message: It's better to use a test set, but if you can't due to computational limits, OOB is a legit alternative. Also, if I tune parameters, it would not feel right to not have independent test data. :» permalink embed save parent report give gold reply [–]Hintonofflax 1 point2 points3 points 4 hours ago (0 children) The question we address here is whether the OOB performance measures can be expected to be reasonably similar to the results we would obtain if we decided to allocate some data to a test partition. The simple answer is that some differences are in fact to be expected for the following reasons: OOB predictions are based on a rather small subsample of the trees in the forest and are thus at a disadvantage relative to predictions that can be legitimately based on the entire forest. We would expect that a prediction based on 500 trees would be (on average) more accurate than one based on a subset of 185 of those trees. OOB predictions are expected to exhibit worse performance because their distribution is not the same as the distribution on which the individual trees are grown. This difference is behind the .632 bootstrap, which estimates the prediction error of a single model as a weighted average of the OOB error rate obtained from repeated bootstrap sampling and the naive error rate based on the training data. See Efron and Tibshirani, (1993), p. 253). As they say, "the bootstrap samples used to compute the [OOB error rate] are further away on the average than a typical test sample, by a factor of 1/.632." In other words, Efron and Tibshirani argue that OOB-based error estimates tend to be pessimistic. Both reasons lead us to expect that OOB results will be pessimistic - but typically only mildly so. A later paper by Dr. Leo Breiman (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.3712&rep=rep1&type=pdf) shows experimental results comparing OOB and test sample error estimates. http://info.salford-systems.com/blog/bid/288278/Random-Forests-OOB-vs-Test-Partition-Performance So while a test set is not absolutely necessary (even when tuning parameters), to get a solid generalization estimate you would still have a test set (also to fairly compare to other algorithms without OOB). permalink embed save report give gold reply [–]JAGGI_JATT 1 point2 points3 points 3 hours ago (0 children) One of the problems with OOB error estimates I have found is that it can be over-optimistic in case the training data is not i.i.d. This can be the case if I train my model with synthetic data say each image with 90,180 and 270 rotations, with features which are near duplicates in some cases, so in reality "the left out trees" are actually trained with these examples, and are not OOB. This has the been the case often with my datasets, where a similar underlying process leads to 3-4 similar examples in my datasets during my training time, and I overestimate OOB accuracy. permalink embed save report give gold reply about blog about source code advertise careers help site rules FAQ wiki reddiquette mod guidelines contact us apps & tools Reddit for iPhone Reddit for Android mobile website buttons <3 reddit gold redditgifts Use of this site constitutes acceptance of our User Agreement and Privacy Policy. © 2017 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology π Rendered by PID 45753 on app-182 at 2017-07-27 21:22:12.986663+00:00 running 8301204 country code: US.
