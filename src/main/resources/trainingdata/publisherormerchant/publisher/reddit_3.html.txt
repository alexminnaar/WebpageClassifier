[D] How do you version control your neural net? : MachineLearning jump to content my subreddits edit subscriptions popular -all -random |  AskReddit -funny -news -todayilearned -worldnews -gifs -pics -videos -movies -gaming -aww -Showerthoughts -mildlyinteresting -nottheonion -Jokes -television -OldSchoolCool -Music -sports -LifeProTips -explainlikeimfive -UpliftingNews -personalfinance -science -EarthPorn -food -TwoXChromosomes -IAmA -books -tifu -space -askscience -photoshopbattles -Futurology -Art -WritingPrompts -Documentaries -GetMotivated -dataisbeautiful -gadgets -nosleep -history -DIY -listentothis -creepy -InternetIsBeautiful -philosophy -blog -announcementsmore »  MachineLearning comments Want to join? Log in or sign up in seconds.| English limit my search to r/MachineLearning use the following search parameters to narrow your results: subreddit:subreddit find submissions in "subreddit" author:username find submissions by "username" site:example.com find submissions from "example.com" url:text search for "text" in url selftext:text search for "text" in self post contents self:yes (or self:no) include (or exclude) self posts nsfw:yes (or nsfw:no) include (or exclude) results marked as NSFW e.g. subreddit:aww site:imgur.com dog see the search faq for details. advanced search: by author, subreddit... this post was submitted on 20 Jul 2017 17 points (80% upvoted) shortlink: remember mereset password login Submit a new link Submit a new text post MachineLearningsubscribeunsubscribe112,934 readers 549 users here now Rules For Posts +Research +Discussion +Project +News @slashML on Twitter AMAs: Google Brain Team (8/11/2016) The MalariaSpot Team (2/6/2016) OpenAI Research Team (1/9/2016) Nando de Freitas (12/26/2015) Andrew Ng and Adam Coates (4/15/2015) Jürgen Schmidhuber (3/4/2015) Geoffrey Hinton (11/10/2014) Michael Jordan (9/10/2014) Yann LeCun (5/15/2014) Yoshua Bengio (2/27/2014) Beginners: Please have a look at our FAQ and Link-Collection Metacademy is a great resource which compiles lesson plans on popular machine learning topics. For Beginner questions please try /r/LearnMachineLearning , /r/MLQuestions or http://stackoverflow.com/ Advanced Courses Related Subreddit : LearnMachineLearning Statistics Computer Vision Compressive Sensing NLP ML Questions /r/MLjobs and /r/BigDataJobs /r/datacleaning /r/DataScience /r/scientificresearch /r/artificial created by kunjaana community for 7 years message the moderators MODERATORS kunjaan cavedavenaive olaf_nij BeatLeJuce about moderation team » discussions in r/MachineLearning <> X 35 · 4 comments [N] MILA Obtains $1.6 Million AI Safety Research Grant 23 · 2 comments [D] How viable? Building a 50 Teraflops AMD Vega Deep Learning Box for Under $3K 21 · 1 comment [P] Spotlight: deep learning recommender model framework in PyTorch 13 · 2 comments [R] Hassabis et. al.: Neuroscience-Inspired Artificial Intelligence 47 · 1 comment [N] An Update to Open Images - Now with Bounding-Boxes · 5 comments [D] What are some solid academic reasons for using Generative Adversarial Networks? · 1 comment [D] The world needs AI researchers. Here’s how to become one. 5 [R] Learning Deep Architectures via Generalized Whitened Neural Networks 70 · 15 comments [N] OpenAI open-sources new policy gradient implementations 5 · 7 comments [P] A Network Structure Visualizer for CNNs (includes dilated/atrous convolution and SAME/VALID padding modes from TensorFlow) 16 17 18 Discusssion[D] How do you version control your neural net? (self.MachineLearning) submitted 23 hours ago by iamwil When I started working with neural nets I instinctively started using git. Soon I realised that git isn't working for me. Working with neural nets seems way more empirical than working with a 'regular' project where you have a very specific feature (e.g. login feature): you create a branch where you implement this feature. Once the feature is implemented you merge with your develop branch and you can move to another feature. The same approach doesn't work with neural nets for me. There's 'only' one feature you want to implement - you want your neural net to generalise better/generate better images/etc (depends on the type of problem you are solving). This is very abstract though. One often doesn't even know what's the solution until you empirically try to tweak several hyper parameters and see the loss function and accuracy. This makes the branch model impossible to use I think. Consider this: you create a branch where you want to use convolutional layers for example. Then you find out that your neural net is performing worse. What should you do know? You can't merge this branch to your develop branch since it's a basically 'dead end' branch. On the other hand when you delete this branch you lose information that you've already tried this model of your net. This also produce huge amount of branches since you have enormous number of combinations for your model (e.g. convolutional layers may yield better accuracy when used with different loss function). I've ended up with a single branch and a text file where I manually log all models I have tried so far and their performance. This creates nontrivial overhead though. 51 comments share report all 51 comments sorted by: best topnewcontroversialoldrandomq&alive (beta) [–]evc123 10 points11 points12 points 13 hours ago* (4 children) Schmidhuber does it for me. Everytime I commit a new architecture variation, Schmidhuber immediately makes a pull request saying the date on which he already invented it. I then use his addendum as the name for the version. permalink embed save report give gold reply [–]iamwil[S] 0 points1 point2 points 5 hours ago (0 children) Haha. Sounds like it hits too close to home. permalink embed save parent report give gold reply [–]alexmlamb 31 points32 points33 points 1 day ago (5 children) main_v1.py main_v2.py rbm_main_v1.py rbm_main_v2.py permalink embed save report give gold reply [–]cognizant_ape 1 point2 points3 points 22 hours ago (0 children) How about the model weights? Do you keep a repeatable training procedure as well? permalink embed save parent report give gold reply [–]iamwil[S] 0 points1 point2 points 21 hours ago (3 children) This can't be the way everyone else does this, as it sounds awful. I assume you're just being flippant. permalink embed save parent report give gold reply [–]Mehdi2277 1 point2 points3 points 15 hours ago (2 children) That was essentially my method. While I use version control more normally for other software projects, for playing with models I haven't really found a useful way to use vc for experiments. Instead I just play around in ipython notebooks. I usually don't bother keeping the code for all the various runs, although it is a good idea to record it somewhere (a word document describing the runs and whether it converged/how well it did is essentially my way). permalink embed save parent report give gold reply [–]Deep_Fried_Learning 1 point2 points3 points 11 hours ago (1 child) A word document! Why didn't I think of that? It's so beautifully simple. permalink embed save parent report give gold reply [–]p_pistol 0 points1 point2 points 8 hours ago (0 children) Lol permalink embed save parent report give gold reply [–]duschendestroyer 5 points6 points7 points 23 hours ago (4 children) https://www.reddit.com/r/MachineLearning/comments/3npg0d/how_to_keep_track_of_experiments/ permalink embed save report give gold reply [–]perspectiveiskey 2 points3 points4 points 21 hours ago (3 children) Wow, that Sacred link is useful!! permalink embed save parent report give gold reply [–]pchalasani 1 point2 points3 points 10 hours ago (2 children) I've been using Sacred with my Pytorch experiments recently. (Interestingly, it comes from Schmidhuber's lab). Overall it does a good job of "watching over my experiments behind my back" and recording results and artifacts that I save, etc. The one downside I found is that I cannot use any of the functions that I designate as "capturing" the config parameters. For example when I "run" an experiment in a Jupyter notebook, and then want to examine a global variable or re-run one of the sub-functions, those are not directly visible any more. Also encapsulating stuff in sacred makes it hard for newcomers to understand your code. I'll give Artemis a try to see if it's any better permalink embed save parent report give gold reply [–]perspectiveiskey 0 points1 point2 points 8 hours ago (1 child) Thanks for that heads up. Curious: do you actually run training sessions in notebooks? Doesn't it take forever? I usually batch run on a terminal and periodically save state, stitch it all into a "video" of sorts to look at when it's all done. permalink embed save parent report give gold reply [–]pchalasani 0 points1 point2 points 8 hours ago (0 children) Some experiments take a while, so I end up losing connection to the remote Jupyter. Later I can re-establish the connection and then look at exp.info() to see how the run ended. permalink embed save parent report give gold reply [–]bbsome 3 points4 points5 points 1 day ago (2 children) I personally keep all neural nets architectures in config files, potentially with some hyper parameters in it which can be tweaked. The config files are similar to what caffee's proto file is, but more high level without having to use 1000 lines to specify the model. Than you just keep a bunch of config files around. permalink embed save report give gold reply [–]iamwil[S] 0 points1 point2 points 23 hours ago (1 child) I assume one config file pertains to one unique combination of models and parameters that was run. How do you organize the config files? How do you know which ones were improvements and which were regressions? What if you have new data or you added/removed features? Then wouldn't some of the older config files not be able to run? permalink embed save parent report give gold reply [–]bbsome 4 points5 points6 points 21 hours ago (0 children) So no a config file contains only model architecture, thus not the hyper parameters. All of the actual experiments have their own folder where there is a smaller config with the exact hyper parameters as well as the results. permalink embed save parent report give gold reply [–]treebranchleaf 2 points3 points4 points 12 hours ago* (0 children) I agree that git is not the right tool to be managing different versions of your experiments. You want to always be able to reproduce any experiment in the master branch. As Mattoss mentioned, there is a python package that helps with this: https://github.com/QUVA-Lab/artemis (disclaimer: we are the authors so of course we say it's great) You define an "experiment function" that runs the experiment. Every time you want to add a feature, you add an argument to that function (where the default is for that feature not to exist at all). (eg dropout_rate=0, ...). You decorate this with the @experiment_function decorator, like @experiment_function
def demo_mnist_mlp(
        minibatch_size = 10,
        learning_rate = 0.1,
        hidden_sizes = [300],
        seed = 1234,
        ....
        ):
    ...
 Then you create "variants" on this experiment: e.g. demo_mnist_mlp.add_variant('full-batch', minibatch_size = 'full', n_epochs = 1000)
demo_mnist_mlp.add_variant('deep', hidden_sizes=[500, 500, 500, 500])
...
 You can "run" the experiment, and its variants. e.g. demo_mnist_mlp.get_variant('deep').run()
 When you run an experiment all console output, figures, and the return value are saved to disk, so the results can be reviewed later. Artemis includes a simple command-line user interface to help you view all your experiments and their results, which you open through from artemis.experiments.ui import browse_experiments
browse_experiments()
 Here's the full example permalink embed save report give gold reply [–]mimighost 2 points3 points4 points 1 day ago (7 children) You can just use git to stage your code. Your model is just code, not very different from regular code. You can always rollback to history if the change is not ideal. permalink embed save report give gold reply [–]iamwil[S] 1 point2 points3 points 23 hours ago (6 children) That doesn't keep a record of what didn't work well that I've tried before. permalink embed save parent report give gold reply [–]mimighost 2 points3 points4 points 22 hours ago (0 children) As to your problem, remember git is essentially a file management tool with history, so say you have a model that doesn't work, just make a history folder, then commit the failed model into it. In this way u log your experiments as well in git while keep your mainline code clean. Your problem is my application level, u can definitely use git to solve it. permalink embed save parent report give gold reply [–]bge0 0 points1 point2 points 23 hours ago (0 children) I generally only commit when i have a working baseline. Until then use a feature branch and git flow permalink embed save parent report give gold reply [–]olBaa 0 points1 point2 points 14 hours ago (2 children) // doesnt work metric 0.1 git push change the code permalink embed save parent report give gold reply [–]iamwil[S] 0 points1 point2 points 14 hours ago (1 child) I assume you're saying that you litter your code base with code that didn't work commented out, and just commit it? permalink embed save parent report give gold reply [–]olBaa 0 points1 point2 points 13 hours ago (0 children) nah, delete/replace it with the next commit. the idea is simply that you can look up the particular part of the history for the keyword permalink embed save parent report give gold reply [–]happymask-salesman 0 points1 point2 points 9 hours ago (0 children) Make a revert commit. permalink embed save parent report give gold reply [–]NicolasGuacamole 2 points3 points4 points 14 hours ago (2 children) There's nothing wrong with dangling branches which don't get merged. I use git. permalink embed save report give gold reply [–]siyideng 1 point2 points3 points 4 hours ago (1 child) +1, and proper tags for the commits that worth highlighting. permalink embed save parent report give gold reply [–]NicolasGuacamole 0 points1 point2 points 4 hours ago (0 children) Yep agreed. Personally I tag stuff based on semantic version numbers and only then, but I can see the utility making more use of them. permalink embed save parent report give gold reply [–]cognizant_ape 1 point2 points3 points 23 hours ago* (3 children) We use Keras to persist the models then zip them up and put them in an S3 bucket for the app and stage. The root "directories" of the bucket define the "namespace" which keeps the library major versions like Keras and tensorflow fixed. A breaking version gets a new namespace. Then the model name and the model version are the rest of the S3 key after the namespace. Since you can list S3 keys by prefix it makes listing all the namespaces in an app, all the models in a namespace and all the versions in a model really easy. Edit: we also zip up meta data about the work that went into the model and how to reproduce it, and a small test set along with expected results to be able to sanity check the model after pulling it down. Also, once you load the model into memory it's trivial to probe it's structure with either Keras or Tensorflow permalink embed save report give gold reply [–]iamwil[S] 0 points1 point2 points 14 hours ago (1 child) Ah. So I assume you wrote a script to upload the models to S3 whenever you 'checkpoint', so the naming convention is consistent? permalink embed save parent report give gold reply [–]cognizant_ape 0 points1 point2 points 13 hours ago (0 children) Exactly, the version is just the datetime. permalink embed save parent report give gold reply [–]NicolasGuacamole 0 points1 point2 points 4 hours ago (0 children) Can you say where you work? permalink embed save parent report give gold reply [–]perspectiveiskey 1 point2 points3 points 21 hours ago (0 children) It's definitely hard. I use pytorch, and I've gotten created a few "base modules" which I use to carry out generic operations on a particular type of network I want. I say "base" in quotes, because I actually use the modules - I'm not inheriting them or anything. Anyways, my base LSTM module, for instance, is designed specifically for time-series work. And I've done some legwork to make sure I can text configure it to a more or less robust interpretation of "text configure". So the modules are stored in git, but the endless revisions to the text configs are not. It's the best I've been able to achieve so far, and I readily recognize it's a technical debt I'm building up. permalink embed save report give gold reply [–]mljoe 1 point2 points3 points 18 hours ago (1 child) Not directly answering your question, but this paper discusses the unique challenges of machine learning development: Machine Learning: The High Interest Credit Card of Technical Debt https://research.google.com/pubs/pub43146.html It says that the tools and procedures used in traditional software development aren't a perfect fit. It goes into why, and has some suggestions on how to do better. permalink embed save report give gold reply [–]iamwil[S] 0 points1 point2 points 15 hours ago (0 children) Ah, thanks. I think I ran across this paper before, but didn't remember it. I'll take a look again. permalink embed save parent report give gold reply [–]dmpetrov 1 point2 points3 points 13 hours ago* (0 children) I usually do not create new branches for experiments. When I feel that I'm on a 'dead end' then I check out a previous version with a new branch creation (otherwise you detach git HEAD and won't be able to commit). $ git log -n 5 # find a right commit then $ git checkout -b back_to_conv_size_tunning d578ae7 It is very convenient not to lose data that you generated on each of these 'dead ends'. You can manually archive these results (and assign creative names to your result files) or use https://dataversioncontrol.com (DVC) tool which does it for you (and assigns regular names with git-hash suffixes). With DVC when you jump back to one of your 'dead ends' by git checkout DVC will transparently replace data file to the right version - this is really cool and helps you to avoid ugly naming like SMASHv1a, SMASHv1b from the above. permalink embed save report give gold reply [–]tgyatso 1 point2 points3 points 9 hours ago (0 children) FWIW, this recently appeared https://medium.com/towards-data-science/how-to-version-control-your-machine-learning-task-cad74dce44c4 I didn't know about DVC, seems like a good alternative to version controlling raw text that controls model parameters. permalink embed save report give gold reply [–]ajmooch 0 points1 point2 points 1 day ago (2 children) I'm a mechanical engineer with no formal software training. When I prototype ideas for a research project, I literally just save multiple copies of my model/training code and keep copious detailed notes on what each version is, both in comments at the top of the script and in a centralized document. So my initial script might be "SMASHv1", with small changes being "SMASHv1a," SMASHv1b," etc. As I'm often running dozens of experiments with each version, I give the logs and model files descriptive names (usually indicating their hyperparameters, like depth, width, #epochs, etc). and save the code that generated the model in the same file as the model, which is all done automatically with some simple but robust boilerplate I've had around for ages. The idea is that if I ever need to return to an old experiment or reproduce a figure, I can just load up the model directly and have everything be just like it was when I ran it, even if I've since moved on by eight or nine versions. Once I'm through the really early "iterate a dozen times a day" stage and running larger experiments (with a more fleshed out idea) I use a simple rule of thumb that if I make an update to the boilerplate or the code that breaks backwards compatibility, I update the version (so every script in the "SMASHv5" folder should be compatible with each other script). This makes for a lot of versions, but so long as I keep my notes up to date it's easy to know where everything is. Mind you this is for research, so what you'd want to do in deployment is probably follow actual good practice and versioning, but this is the workflow I've sort of naturally fallen into. Probably worth mentioning that thus far I've worked entirely alone without external input, so if you have other team members or people who see your work before it's a complete report/paper, this approach is probably bad. permalink embed save report give gold reply [–]iamwil[S] 0 points1 point2 points 23 hours ago (1 child) What would be considered the difference between "SMASHv1a" and "SMASHv2"? How big of a difference do you need to increment the major number, rather than using a subset letter (like 1a)? Ah, so basically, your boilerplate utilities is versioned to guarantee all the experiments written in that particular version (say SMASHv1) will all still execute, regardless of any subsequent changes to the boilerplate in later experiments. Why would it be bad for teams? It seems like it's mostly organized, if the central notes has a reference to where everything is? Also, I didn't realize mechanical engineers did machine learning! permalink embed save parent report give gold reply [–]ajmooch 1 point2 points3 points 23 hours ago (0 children) How big of a difference do you need to increment the major number, rather than using a subset letter (like 1a)? Basically breaking changes, or any change that's large enough to make my brain think of the new code as a wholly separate entity from is predecessor. For example, in my nets I have many dozens of small paths; SMASHv4 uses paths with a fixed number of channels per path, but SMASHv5 allows the number of channels per path to vary. This fundamentally changes the way the whole apparatus operates, and while it only ended up being about forty lines of code change, I think of the two versions as "different" in my head, and so I upped the version number. This also did happen to break some backwards compatibility with earlier versions, further warranting the upgrade. Any SMASHv5a, v5b, v5c would just be smaller changes, e.g. "In this version, the model uses ReLU-BN-CONV instead of BN-ReLU-CONV." Why would it be bad for teams? It might not be, but it's inconsistent with my experience using git/svn in a small team. Like I said, I'm not a computer scientist by training, so I'm not really hip to good versioning practices yet. The main thing is that these breaking changes happen pretty frequently and my code is only as modular as it has to be to maximize the speed of iteration; while this may be a "fact of life" for research code, I think that if I was working closely with someone it would make more sense to structure things around improving modularity to facilitate teamwork, even if that slows you down a bit as an individual. I didn't realize mechanical engineers did machine learning! Aside from some ill-advised neural nets for low level control, we don't. I'm a nerd. permalink embed save parent report give gold reply [–]trnka 0 points1 point2 points 21 hours ago (3 children) Commit all on main. Log raw results externally in system of your choice (even google sheets is fine). Summarize trends weekly/monthly in internal blogs/reports. The main problem I have is that the options for feature engineering/preprocessing and hyperparams only seem to increase over time. permalink embed save report give gold reply [–]iamwil[S] 0 points1 point2 points 21 hours ago (2 children) What do you mean "options for feature engineering/preprocessing and hyperparams only seem to increase over time"? Can you elaborate a bit more? permalink embed save parent report give gold reply [–]trnka 1 point2 points3 points 17 hours ago (1 child) I mean that I don't think to log all hyperparams initially. For instance, I might realize later on that my settings for early stopping are important. But I don't have the values filled in for old data. Sometimes I'm good about filling in the old values once I start to tweak them but oftentimes I'm lazy and just copy/paste the dict from a random search. I guess what I mean is that early on I don't know which settings are going to be relevant and I'm not good at making sure 100% of hyperparameters/feature engineering are documented. Or I add various forms of scaling and/or outlier removal over time and it's tough to remember what the defaults were even earlier in the day. permalink embed save parent report give gold reply [–]tryndisskilled 0 points1 point2 points 13 hours ago (0 children) I think this is the culprit. No matter how well you want to organize things from the get go, it's nearly impossible that you won't get to change it in the future: which means no backwards compatibility. However, I think we are forgetting that such projects are basically research projects: trying a bunch of things (most of which will fail), learning new ones on the way, forgetting some... There might be no perfect way to manage your versioning. permalink embed save parent report give gold reply [–]Mattoss 0 points1 point2 points 16 hours ago (0 children) I can recommend https://github.com/QUVA-Lab/artemis as a convenient way to organize your experiments. Plus it gives you plotting, file management and a few other useful things you can choose to use. permalink embed save report give gold reply [–]woadwarrior 0 points1 point2 points 12 hours ago (0 children) I use git LFS to version my model weights alongside the code. permalink embed save report give gold reply [–]Ithm 0 points1 point2 points 7 hours ago (0 children) Unit tests and version control permalink embed save report give gold reply [–]AoeAoe 0 points1 point2 points 4 hours ago (0 children) Can't see the problem with you current approach, just create a branch and if it turns out it's not what you want checkout master and try something else. permalink embed save report give gold reply [–]_MandelBrot 0 points1 point2 points 1 day ago (2 children) Creating statistical models does have quite a lot of differences to traditional software engineering. In plan-based development you goal is to implement modular features, following a specification of your client - usually composed of several concrete requirements. You don't have that for neural networks. Like you pointed out, their design is extremely horizontal and thus not really suited for a vertical development process. (Just my 0.02$, I've never considered git for my projects) permalink embed save report give gold reply [–]iamwil[S] 1 point2 points3 points 23 hours ago (1 child) What have you done to keep a record of the different variations you've tried when building statistical models? permalink embed save parent report give gold reply [–]_MandelBrot 1 point2 points3 points 23 hours ago (0 children) PRJ1 .log .config data [...] NN1 nn1.py results (folder) NN2 nn2.py results (folder) NN3 nn3.py results (folder) I write summaries of the project into the .log file. The nnx.py's are referencing libraries I wrote and are basically like config files. The results folder in every model version contains performance and some neat pictures. The data is usually in PRJ1/data. Datasets can change (I could remove/add features, change dimensions), I could add alternative validation/test sets. All that information is in the .config file. That works pretty well permalink embed save parent report give gold reply about blog about source code advertise careers help site rules FAQ wiki reddiquette mod guidelines contact us apps & tools Reddit for iPhone Reddit for Android mobile website buttons <3 reddit gold redditgifts Use of this site constitutes acceptance of our User Agreement and Privacy Policy. © 2017 reddit inc. All rights reserved. REDDIT and the ALIEN Logo are registered trademarks of reddit inc. Advertise - technology π Rendered by PID 50788 on app-95 at 2017-07-21 21:36:14.769849+00:00 running d5ad74e country code: US.
